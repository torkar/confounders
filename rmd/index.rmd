---
title: "Replication package"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    number_sections: true
bibliography: references.bib
---

# Introduction

This document can be used to replicate the results from the manuscript on confounders. First, we provide a small example that serves as a simple case to build intuition. Second, we provide a larger example where we conduct a sensitivity analysis of unobserved/unknown confounders.

Let's start by making sure we have some needed packages.

```{r, message=FALSE}
# Package names
packages <- c("ggplot2", "brms", "ggdag", "cmdstanr", "tipr", "broom", "broom.mixed", "tidyverse", "dagitty", "bayesfam", "fastDummies")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
  # bayesfam you need to install via GitHub...
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
theme_set(theme_minimal())
# If you want to run with Stan and use MCMC then you need to install cmdstan
# > cmdstanr::install_cmdstan(cores=8)
# in R.
```


# A first example to build intuition

## The effect of a confounder

A confounder ($Z$) is a variable that affects both the treatment ($X$) and the outcome ($Y$), and when a confounder exists we get a spurious (fake) association. In our work we follow the notation of Pearl and claim that confounding is a purely causal concept [@pearl09reason].

We need to distinguish between two types of studies common in software engineering: experiments (randomized controlled trial, RCT) and observational studies. One reason for why experiments are considered gold standard when conducting studies is that confounding is supposed to be a non-issue due to randomized allocation of treatments (and selection of sample), i.e., very simply put, if we randomly allocate the treatment ($X$), then the causal effect of the confounder ($Z$) will not matter. 

A, perhaps, more straightforward way to think about confounding is to contrast experiments with observational studies as 

> [$\ldots$] any context in which the association between an outcome $Y$ and a predictor of interest $X$ 
> is not the same as it would be, if we had experimentally determined the values of $X$.
[@mcelreath20statrethinking]{style="float:right"}

A graphical summary (directed acyclic graph), of what we said above, can be visualized like this:

```{r, out.width="60%", fig.align="center"}
myDAG <- dagify(
  X ~ Z,
  Y ~ Z,
  Y ~ X,
  exposure = "X", # treatment
  outcome = "Y",
  coords = list(
    x = c(X = 1, Z = 2, Y = 3),
    y = c(X = 1, Z = 2, Y = 1)
  )
)

ggdag(myDAG) + theme_dag()
```

In the plot above we see that the confounder $Z$ has a causal effect on the treatment $X$ and the outcome $Y$. Also, $X$ affects $Y$, but that is what we're generally speaking interested in estimating. 

Confounders are scary, but no need to panic. If we condition on a confounder (i.e., include it as a predictor in our model) we close the path $\overrightarrow{X \leftarrow Z \rightarrow Y}$, and hence an unbiased estimate of the direct effect of $X$ on $Y$ can be estimated. However, this means that we must have measured $Z$, so we can condition on that variable. As we will see later, omitted variable bias (i.e., we do not have access to some variables) is something we can reason about and provide convincing arguments that it likely does not affect the results of a study. But how dangerous is omitted variable bias?

Let's generate some fake data first. That way we *know* the truth.

```{r, message=FALSE, cache=TRUE}
N <- 1e5
z <- rnorm(N) # exogenous 
x <- 0.5 * z + rnorm(N) # endogenous
y <- 0.3 * z + 0.4 * x + rnorm(N) # endogenous; set x = 0.4

d <- data.frame(
  z = z,
  x = x,
  y = y
)

# next run two models, one where we do not condition on z, and one where we do condition on z
withoutZ <- brm(
  y ~ x,
  data = d,
  refresh = 0
)

withZ <- brm(
  y ~ x + z,
  data = d,
  refresh = 0
)
```

Disregarding the estimate for $Z$ (we're seldom interested in the causal effect of a confounder, but there are exceptions), the estimates we have of $X$, i.e., $\hat{x}$, is what interests us.

```{r}
fixef(withZ)[2,1]
fixef(withoutZ)[2,1]
```

Conditioning on $Z$ allows us to get the correct estimate for $X$ ($\hat{x}_z \approx$ `r round(fixef(withZ)[2,1], 2)`) since we close the backdoor. If we don't condition on $Z$ we get a biased (in this case positive) estimate, i.e., $\hat{x}_{\neg z} \approx$ `r round(fixef(withoutZ)[2,1], 2)`, because of the confounder's effect on both $X$ and $Y$.

There are a few lessons learnt from the above example. First, confounders can have an effect on the estimate. Second, the bias is positive in this case, but can also be negative (change the confounder's sign on $X$, i.e., from $0.5$ to $-0.5$ instead and you'll see). Third, if we want to have an unbiased estimate we need to condition on the confounder ($Z$ in this case), to close the backdoor that goes from $\overrightarrow{X \leftarrow Z \rightarrow Y}$.

But if we don't have access to $Z$, i.e., it's unmeasured for some reason, or perhaps unknown, then we are facing omitted variable bias. However, all is not lost. *We can still argue if it's likely that a confounder would nullify the causal effect of $X$ on $Y$*.

## Accounting for the unknown^[A more involved example can be found at https://evalf21.classes.andrewheiss.com/example/confounding-sensitivity/#you-can-never-close-every-backdoor-without-an-experiment] 

If we look at the DAG we introduced previously, we see that there are two arrows going from $Z$. First, we have $Z \rightarrow X$ and then we have $Z \rightarrow Y$. These two paths, when we want to look at unobserved effects, are dependent on each other. If we want to assess the effect of $Z$ on $X$ in these type of analyses, one often talks about the Standardized Mean Difference (SMD).

The SMD is the confounder's effect on the treated and the non-treated groups in $X$. In math, something like this:

$$\mathrm{SMD} = \frac{\bar{Z}_t - \bar{Z}_{\neg t}}{\sigma_Z}$$

The difference between the treated $\bar{Z}_t$ and the untreated $\bar{Z}_{\neg t}$ can then, by using SMD, be represented as a difference in $\sigma$. For example, a $3\sigma$, $1\sigma$, or $0.1\sigma$ difference between the treated and untreated groups. 

If we next look at the other arrow ($Z \rightarrow Y$), we don't need to think about the SMD since $Z$ is affecting the outcome $Y$ and not the treatment $X$. Here we can simply estimate the effect as a regular $\beta$ estimate that we're used to, i.e., in our case the $\beta_Z$ estimate of our confounder $Z$. As is common, one could for example say that $\beta_Z = 3$ would mean that a 1-unit change in $Z$ would imply a 3-unit change in $Y$.

With these two concepts, the SMD and the $\beta_Z$ estimate, we can now add assumptions to our analysis and argue how likely it is that they would hold. According to @linPK98sens we now have three options to choose between:

* Specify $\beta_Z$ and estimate how large SMD needs to be to cancel out the direct effect of $X$ on $Y$.
* Specify SMD and estimate how large a $\beta_Z$ needs to be to cancel out the direct effect of $X$ on $Y$.
* Finally, by specifying both SMD and $\beta_Z$ we can investigate the number of confounders, $n_c$, needed to cancel out the direct effect of $X$ on $Y$.

Next, we will focus on the first two analyses, since they are more common.

### Assumptions concerning $\beta_Z$
If we continue using the analysis we did initially we can collect the true effect that was estimated (which we know was $0.4$) and assume $\beta_Z = 1.5$, i.e., a 1-unit change in $Z$ would imply a 1.5-unit change in $Y$:

```{r}
tidy(withZ, conf.int=TRUE) |>
        filter(term == "x") |>
        pull(estimate) |>
        tip(confounder_outcome_effect = 1.5)
```

then a hypothetical unmeasured continuous confounder with $\beta_Z = 1.5$ and $\mathrm{SMD} \approx -2.3$, would cancel out the effect ($\beta_X = 0.4$) of $X$ on $Y$. Hence, given our assumptions, a 1-unit change in an unmeasured $Z$ leading to a 1.5-unit change in $Y$ and, *et voil√†*, you would not have a Nature paper. Making it more concrete might help.

Say that the confounder $Z$ is number of hours spent on implementing a feature (unmeasured confounder) with $\mu = 45$ and $\sigma = 1.5$. Our standardized difference of $-2.3$ can be transformed to hours by multiplying with our $\sigma = 1.5$, i.e., $1.5 \cdot -2.3 \approx -3.5$. This implies that between treatment and non-treatment we would need to see a difference of approximately $3.5$ hours when implementing a feature. Is this unlikely? Perhaps; context matters. However, as an empirical researcher you might not want to bet your life on it if you look at the plot below; this is how large a difference there would be between the two populations. More importantly, you should present these results to your readers and argue for why it is unlikely that a counfounder exists, or go measure it carefully.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
tip_z <- tibble(x = sample(0:1, N, replace = TRUE)) |> 
  mutate(z = rnorm(N, mean = 45 + (-2.3 * 1.5 * x), sd = 1.5))

tip_z_avg <- tip_z |>
  group_by(x) |>
  summarize(avg = mean(z))

ggplot(tip_z, aes(x = z, fill = factor(x))) +
  geom_density(alpha = 0.5) + 
  geom_vline(data = tip_z_avg, aes(xintercept = avg, color = factor(x))) + 
  theme(legend.position = "none")
```

### Assumptions concerning SMD

As previously explained, there are two arrows going from $Z$. In the previous section we focused on the arrow $Z \rightarrow Y$, i.e., setting the $\beta_Z$ estimate to analyze the size of SMD needed to cancel out our causal effect. Here we will now do the opposite, i.e., given an SMD, how large a $\beta_Z$ is needed to cancel out the causal effect of $X \rightarrow Y$.

For the sake of completness we'll assume $\mathrm{SMD} = -2.3$ to validate the result in the previous section, i.e., we should end up with $\beta_Z \approx 1.5$.

```{r}
tidy(withZ, conf.int=TRUE) |>
        filter(term == "x") |>
        pull(estimate) |>
        tip(exposure_confounder_effect = -2.3)
```

Above we see that $\beta_Z \approx 1.50$ (i.e., the confounder_outome_effect), thus we have an indication that this works both ways. Given that we have introduced two distinct concepts used to estimate unobserved/unknown confounders, we can now turn out attention to a more involved example.

# A second, more realistic, example

```{r, echo=FALSE, out.width="60%", fig.align="center"}
var_labs <- c(
  `P` = "P",
  `BA` = "BA",
  `H` = "H",
  `PL` = "PL",
  `L` = "L",
  `O` = "O",
  `PT` = "PT",
  `S` = "S",
  `TS` = "TS"
)

col_labs <- c(
  `P` = "black", `BA` = "black", `H` = "grey60", `PL` = "grey60",
  `L` = "grey60", `O` = "grey60", `PT` = "grey60", `S` = "grey60",
  `TS` = "grey60"
)

udag <- ggdag::dagify(
  H ~ BA,
  P ~ BA,
  P ~ H,
  PL ~ H,
  P ~ L,
  BA ~ O,
  L ~ O,
  P ~ O,
  P ~ PL,
  P ~ PT,
  P ~ S,
  TS ~ S,
  P ~ TS,
  exposure = "BA",
  outcome = "P"
)

df <- udag %>%
  node_canonical(seed = 7) %>%
  mutate(var_labs = var_labs[name], cols = col_labs[name])

ggplot(data = df, mapping = aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(mapping = aes(colour = name), show.legend = FALSE) +
  geom_dag_edges_link() +
  scale_colour_manual(values = df$data$cols) +
  geom_dag_text(mapping = aes(label = var_labs), parse = TRUE) + 
  theme_dag()
```

In [@feldtSFT23sysrev], the authors extracted causal links from several primary studies and assembled disparate evidence into a Unified Directed Acyclic Graph (UDAG). The purpose was to, in the future, analyze the direct effect of Business Area ($BA$) on Productivity, ($P$), i.e., Business Area can be seen as a "treatment" and Productivity as the outcome. 

If one would want to design the next observational study investigating this particular outcome and treatment, the idea would be that one could use the UDAG to know what variables to collect; after all, some studies contained some variables, but no study contained all variables in the UDAG. However, the UDAG, as we will see, can also be used for sensitivity analysis of unknown confunders. That would allow a future study to either be more sure of not facing omitted variable bias from start, i.e., the UDAG is likely to be correct, or to make sure to collect variables that are currently unobserved, i.e., the UDAG needs to be complemented with new variables.

The above UDAG is clearly a more involved example and contains several of the elemental constructs one can find in a DAG. Organization ($O$) is a confounder since it points to the treatment and the outcome. Hardware ($H$) is part of a pipe since $BA \rightarrow H \rightarrow P$. The lower part also contains several ancestors of the outcome. Additionally, we also see that there are no colliders (e.g., $X \rightarrow Z \leftarrow Y$) in the UDAG. So, this seems to be a simple task. Let's see what we should condition on by calculating the adjustment set for the UDAG :

```{r}
adjustmentSets(udag, effect = "direct")
```

It seems our adjustment set is $\mathcal{A} = \{ H, O \}$. By conditioning on $H$ and $O$ (i.e., adjusting for $H$ and $O$) we will be able to estimate the direct effect of $BA$ on $P$. But what if there are unknown confounders? It's not unlikely given that the studies that contributed to the UDAG are all observational. 

Let's do this systematically and overlay numbers on the plot to make sure that we don't miss anything. First, we focus on the nine edges in the upper part of the UDAG since they are all connected to $BA$ directly or indirectly without needing to go through $P$.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
ggplot(data = df, mapping = aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(mapping = aes(colour = name), show.legend = FALSE) +
  geom_dag_edges_link() +
  scale_colour_manual(values = df$data$cols) +
  geom_dag_text(mapping = aes(label = var_labs), parse = TRUE) +
  annotate("text", x = 0, y = 1.17, label = "1") +
  annotate("text", x = 0.9, y = 1.2, label = "2") +
  annotate("text", x = 1.75, y = 0.7, label = "3") +
  annotate("text", x = 1.3, y = -0.1, label = "4") +
  annotate("text", x = -0.4, y = -0.15, label = "5") +
  annotate("text", x = 0.87, y = 0.4, label = "9") +
  annotate("text", x = 0.4, y = 0.4, label = "8") +
  annotate("text", x = -0.8, y = 0.6, label = "6") +
  annotate("text", x = -0.13, y = 0.3, label = "7") +
  theme_dag()
```

If we look at the plot above, and take Edge 1 as an example, the question we're asking is: 

> If there would be a confounder $C$ between $H$ and $BA$, i.e. $H \leftarrow C \rightarrow BA$, how would that affect our possibility to estimate the direct effect of $BA$ on $P$?

The potential answer to the above question would be one of three: 

1. It would not affect the estimate (n/a).
2. By conditioning on other variable(s) we close the path and then the confounder will not affect the estimate (co).
3. We truly have a case of omitted variable bias, which means that we need to conduct a sensitivity analysis (ovb).

In the first case (n/a), a confounder would not affect the possibility for us to receive an unbiased causal estimate of the direct effect. In the second case (co), the confounder would affect our possibility to get an unbiased estimate; however, if we condition on $1,\ldots,n$ other variables, the confounder would no longer affect the results and thus fall under the n/a category. Finally, the third case (ovb), is the one we would not like to see. Here a potential confounder would affect our possibility to get an unbiased estimate. This means that we either need to add additional assumptions and argue that this confounder is unlikely to exist, or think harder, identify such a confounder and then measure it.

For each edge in the plot above, let's analyze what it would imply if an unknown confounder truly existed:

1. n/a
2. n/a
3. co $L$
4. n/a
5. n/a
6. co $PL$
7. ovb
8. ovb
9. n/a

How did we come to the results above? Well, analyzing the UDAG according to Pearl's causal calculus [@pearl09reason] leads to the above results and with experience one receives intuition when analyzing DAGs. However, one can also use software to get some help, e.g., the [dagitty](https://dagitty.net) package in R. If you are so inclined you can verify [online](https://dagitty.net/dags.html?id=P8mw_j) that our assessment above is correct.

To summarize, we can make life easier by also conditioning on $PL$ and $L$, i.e., $\mathcal{A} = \{ H, O, PL, L \}$. However, we still have a challenge dealing with Edges 7 and 8. If an unknown confounder exists between the two nodes on each of those two edges, then we need to fall back on our sensitivity analysis of unknown confounders. However, before we do that, let's first look at the bottom half.

First, if there's a confounder $PT \leftarrow C \rightarrow BA$ or $PT \leftarrow C \rightarrow H$ then we are in trouble. But if we condition on $PT$ we close these paths and even gain higher accuracy in our causal estimate, since $PT$ is an ancestor of the outcome $P$. Hence, we should add $PT$ to our adjustment set. Next, imagine there's a confounder $S \leftarrow C \rightarrow BA$ or $S \leftarrow C \rightarrow H$. That would make it impossible for us to estimate the causal effect correctly. However, as in the previous case, conditioning on $S$ will close that path. Hence, $S$ should be added to our adjustment set. Finally, $TS$ should also be added to our adjustment set, for the same reasons as in the previous two cases.

Ultimately, this implies that if we want to estimate the direct causal effect of $BA$ on $P$, we need to condition on 

$\mathcal{A} = \{ H, O, PL, L, PT, S, TS \}$

to account for possible unknown confounders. However, remember, there's still a challenge in dealing with Edges 7 and 8, i.e., in these two cases we need to analyze how large a confounder needs to be to nullify the direct causal effect of $BA$ on $P$. Let's do just that.

## Model design and execution

Generate data, set diff sizes for direct effect (check sizes and signs in primary studies), and then do sensitivity analysis of the confounders to see if we should go out and hunt for more data before running the study.


* Business Area (BA) -> Categorical distribution; <=5 cat
* Hardware (H) -> Categorical distribution; <=6 cat
* Organization (O) -> Categorical distribution; <=4 cat
* Programming Language (PL) -> Categorical distribution; <=7 cat
* Location (L) -> Categorical distribution; <=10 cat
* Project Type (PT) -> Categorical distribution; <=3 cat
* Size (S) -> Normal distribution; standardized LOC
* Team Size (TS) -> Normal distribution; logNormal(6, 2)
* P -> Productivity is the unit interval [0,1], i.e., Beta(link = "logit", link_phi = "log")

```{r}
#beta <- 0.5
#n <- 100
#x1 <- rnorm(n)
#pr <- inv_logit_scaled(1 + beta * x1)
#y <- rbeta_mean(n, pr, 1) # use mean parameterization for Beta from bayesfam pkg
#d <- data.frame(y = y, x = x1)
#brm(y ~ x, data = d, family = Beta())

n <- 250

c0 <- 0.07
c1 <- 0.1
c2 <- -0.15
c3 <- -0.03
c4 <- 0.9

aHat <- rep(NA, n)
bHat <- rep(NA, n)
cHat <- rep(NA, n)
dHat <- rep(NA, n)
eHat <- rep(NA, n)

# run once so we can save the compile model
x <- sample(x=c("A","B", "C", "D", "E"), 
              size=n, replace=TRUE, prob=rep(1/5, 5))
m_x <- as.matrix(dummy_cols(x)[,-1])
pred <- cbind(1, m_x %*% c(c0, c1, c2, c3, c4))
pr <- inv_logit_scaled(pred[,2])
y <- rbeta_mean(n, pr, 1)
d <- data.frame(y = y, x = x)

p <- get_prior(bf(y ~ x, phi ~ 1), data = d, family = Beta)
p[1,1] <- "normal(0,1)"
p[6,1] <- "normal(0,2)"
p[7,1] <- "normal(0,2)"
m <- brm(bf(y ~ x, phi ~ 1), data = d, prior = p, family = Beta)

# now simulate x times
for(i in 1:500) {
  x <- sample(x=c("A","B", "C", "D", "E"), 
              size=n, replace=TRUE, prob=rep(1/5, 5))
  m_x <- as.matrix(dummy_cols(x)[,-1])
  pred <- cbind(1, m_x %*% c(c0, c1, c2, c3, c4))
  pr <- inv_logit_scaled(pred[,2])
  y <- rbeta_mean(n, pr, 1)
  d <- data.frame(y = y, x = x)

  m <- update(m, newdata = d, family = Beta)

  aHat[i] <- fixef(m)[1]
  bHat[i] <- fixef(m)[1] + fixef(m)[3]
  cHat[i] <- fixef(m)[1] + fixef(m)[4]
  dHat[i] <- fixef(m)[1] + fixef(m)[5]
  eHat[i] <- fixef(m)[1] + fixef(m)[6]
}

round(c(
    c0 = mean(aHat), # 0.07
    c1 = mean(bHat), # 0.1
    c2 = mean(cHat), # -0.15
    c3 = mean(dHat), # -0.03
    c4 = mean(eHat)  # 0.9
    ), 3)
```

## Sensitivity analysis of unknown confounders

# References
