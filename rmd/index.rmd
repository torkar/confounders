---
title: "Replication package"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    number_sections: true
header-includes:
   - \usepackage{amssymb}
bibliography: references.bib
---

# Introduction

This document can be used to replicate the results from the manuscript on confounders. First, we provide a small example that serves as a simple case to build intuition. Second, we provide a larger example where we conduct a sensitivity analysis of unobserved/unknown confounders.

```{r, include=FALSE}
# Package names
packages <- c("ggplot2", "brms", "ggdag", "cmdstanr", "tipr", "broom", "broom.mixed", "tidyverse", "dagitty", "bayesfam", "simcausal", "rethinking")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
  # bayesfam you need to install via GitHub...
  # https://github.com/sims1253/bayesfam
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
# theme_set(theme_minimal())
options(simcausal.verbose = FALSE)
# If you want to run with Stan and use MCMC then you need to install cmdstan in R:
# > cmdstanr::install_cmdstan(cores=8)
```

# A first example to build intuition

## The effect of a confounder

A confounder ($C$) is a variable that affects both the treatment ($X$) and the outcome ($Y$), and when a confounder exists we get a spurious (fake) association. In our work we follow the notation of Pearl and claim that confounding is a purely causal concept [@pearl09reason].

We need to distinguish between two types of studies common in software engineering: experiments (randomized controlled trial, RCT) and observational studies. One reason for why experiments are considered gold standard when conducting studies is that confounding is supposed to be a non-issue due to randomized allocation of treatments (and selection of sample), i.e., very simply put, if we randomly allocate the treatment ($X$), then the causal effect of a confounder ($C$) will not matter. 

A, perhaps, more straightforward way to think about confounding is to contrast experiments with observational studies as 

> [$\ldots$] any context in which the association between an outcome $Y$ and a predictor of interest $X$ 
> is not the same as it would be, if we had experimentally determined the values of $X$.
[@mcelreath20statrethinking]{style="float:right"}

A graphical summary (directed acyclic graph), of what we said above, can be visualized like this:

```{r, out.width="60%", fig.align="center"}
myDAG <- dagify(
  X ~ C,
  Y ~ C,
  Y ~ X,
  exposure = "X", # treatment
  outcome = "Y",
  coords = list(
    x = c(X = 1, C = 2, Y = 3),
    y = c(X = 1, C = 2, Y = 1)
  )
)

ggdag(myDAG) + theme_dag()
```

In the plot above we see that the confounder $C$ has a causal effect on the treatment $X$ and the outcome $Y$. Also, $X$ affects $Y$, but that is what we're generally speaking interested in estimating. 

Confounders are scary, but no need to panic. If we condition on a confounder (i.e., include it as a predictor in our model) we close the path $\overrightarrow{X \leftarrow C \rightarrow Y}$ and, hence, an unbiased estimate of the direct effect of $X$ on $Y$ can be estimated. However, this means that we must have measured $C$, so we can condition on that variable. As we will see later, omitted variable bias (i.e., we do not have access to some variables) is something we can reason about and provide convincing arguments that it likely does not affect the results of a study. But how dangerous is omitted variable bias?

Let's generate some fake data first. That way we *know* the truth.

```{r, message=FALSE, cache=TRUE}
N <- 1e5
c <- rnorm(N) # exogenous 
x <- 0.5 * c + rnorm(N) # endogenous
y <- 0.3 * c + 0.4 * x + rnorm(N) # endogenous; set x = 0.4

d <- data.frame(
  c = c,
  x = x,
  y = y
)

# next run two models, one where we don't condition on c, and one where we do condition on c
withoutC <- brm(
  y ~ x,
  data = d,
  refresh = 0
)

withC <- brm(
  y ~ x + c,
  data = d,
  refresh = 0
)
```

Disregarding the estimate for $C$ (we're seldom interested in the causal effect of a confounder, but there are exceptions), the estimates we have of $X$, i.e., $\hat{x}$, is what interests us.

```{r}
fixef(withC)[2,1]
fixef(withoutC)[2,1]
```

Conditioning on $C$ allows us to get the correct estimate for $X$ ($\hat{x}_c \approx$ `r round(fixef(withC)[2,1], 2)`) since we close the backdoor. If we don't condition on $C$ we get a biased (in this case positive) estimate, i.e., $\hat{x}_{\neg c} \approx$ `r round(fixef(withoutC)[2,1], 2)`, because of the confounder's effect on both $X$ and $Y$.

There are a few lessons learnt from the above example. First, confounders can have an effect on the estimate. Second, the bias is positive in this case, but can also be negative (change the confounder's sign on $X$, i.e., from $0.5$ to $-0.5$ instead and you'll see). Third, if we want to have an unbiased estimate we need to condition on the confounder ($C$ in this case), to close the backdoor that goes from $\overrightarrow{X \leftarrow C \rightarrow Y}$.

But if we don't have access to $C$, i.e., it's unmeasured for some reason, or perhaps unknown, then we are facing omitted variable bias. However, all is not lost. *We can still argue if it's likely that a confounder would nullify the causal effect of $X$ on $Y$*.

## Accounting for the unknown^[A more involved example can be found at https://evalf21.classes.andrewheiss.com/example/confounding-sensitivity/#you-can-never-close-every-backdoor-without-an-experiment] 

If we look at the DAG we introduced previously, 

```{r, echo=FALSE, out.width="60%", fig.align="center"}
myDAG <- dagify(
  X ~ C,
  Y ~ C,
  Y ~ X,
  exposure = "X", # treatment
  outcome = "Y",
  coords = list(
    x = c(X = 1, C = 2, Y = 3),
    y = c(X = 1, C = 2, Y = 1)
  )
)

ggdag(myDAG) + theme_dag()
```

we see that there are two arrows going from $C$. First, we have $C \rightarrow X$ and then we have $C \rightarrow Y$. These two paths, when we want to look at unobserved effects, are dependent on each other. If we want to assess the effect of $C$ on $X$ in these type of analyses, one often talks about the Standardized Mean Difference (SMD).

The SMD is the confounder's effect on the treated and the non-treated groups in $X$. In math, something like this:

$$\mathrm{SMD} = \frac{\bar{C}_t - \bar{C}_{\neg t}}{\sigma_Z}$$

The difference between the treated $\bar{C}_t$ and the untreated $\bar{C}_{\neg t}$ can then, by using SMD, be represented as a difference in $\sigma$. For example, a $3\sigma$, $1\sigma$, or $0.1\sigma$ difference between the treated and untreated groups. 

If we next look at the other arrow ($C \rightarrow Y$), we don't need to think about the SMD since $C$ is affecting the outcome $Y$ and not the treatment $X$. Here we can simply estimate the effect as a regular $\beta$ estimate that we're used to, i.e., in our case the $\beta_C$ estimate of our confounder $C$. As is common, one could for example say that $\beta_C = 3$ would mean that a 1-unit change in $C$ would imply a 3-unit change in $Y$.

With these two concepts, the SMD and the $\beta_C$ estimate, we can now add assumptions to our analysis and argue how likely it is that they would hold. According to @linPK98sens we now have three options to choose between:

* Specify $\beta_C$ and estimate how large SMD needs to be to cancel out the direct effect of $X$ on $Y$.
* Specify SMD and estimate how large a $\beta_C$ needs to be to cancel out the direct effect of $X$ on $Y$.
* Finally, by specifying both SMD and $\beta_C$ we can investigate the number of confounders, $n_c$, needed to cancel out the direct effect of $X$ on $Y$.

Next, we will focus on the first two analyses, since they are more common.

### Assumptions concerning $\beta_C$
If we continue using the analysis we did initially we can collect the true effect that was estimated (which we know was $0.4$) and assume $\beta_C = 1.5$, i.e., a 1-unit change in $C$ would imply a 1.5-unit change in $Y$:

```{r}
tidy(withC, conf.int=TRUE) |>
        filter(term == "x") |>
        pull(estimate) |>
        tip(confounder_outcome_effect = 1.5)
```

then a hypothetical unmeasured continuous confounder with $\beta_C = 1.5$ and $\mathrm{SMD} \approx -2.3$, would cancel out the effect ($\beta_X = 0.4$) of $X$ on $Y$. Hence, given our assumptions, a 1-unit change in an unmeasured $C$ leading to a 1.5-unit change in $Y$ and, *et voil√†*, you would not have a Nature paper. Making it more concrete might help.

Say that the confounder $C$ is number of hours spent on implementing a feature (unmeasured confounder) with $\mu = 45$ and $\sigma = 1.5$. Our standardized difference of $-2.3$ can be transformed to hours by multiplying with our $\sigma = 1.5$, i.e., $1.5 \cdot -2.3 \approx -3.5$. This implies that between treatment and non-treatment we would need to see a difference of approximately $3.5$ hours when implementing a feature. Is this unlikely? Perhaps; context matters. However, as an empirical researcher you might not want to bet your life on it if you look at the plot below; this is how large a difference there would be between the two populations. More importantly, you should present these results to your readers and argue for why it is unlikely that a counfounder exists, or go measure it carefully.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
tip_c <- tibble(x = sample(0:1, N, replace = TRUE)) |> 
  mutate(c = rnorm(N, mean = 45 + (-2.3 * 1.5 * x), sd = 1.5))

tip_c_avg <- tip_c |>
  group_by(x) |>
  summarize(avg = mean(c))

ggplot(tip_c, aes(x = c, fill = factor(x))) +
  geom_density(alpha = 0.5) + 
  geom_vline(data = tip_c_avg, aes(xintercept = avg, color = factor(x))) + 
  scale_fill_grey() + theme_classic() +
  theme(legend.position = "none") +
  labs(y= "", x = "hours")
```

### Assumptions concerning SMD

As previously explained, there are two arrows going from $C$. In the previous section we focused on the arrow $C \rightarrow Y$, i.e., setting the $\beta_C$ estimate to analyze the size of SMD needed to cancel out our causal effect. Here we will now do the opposite, i.e., given an SMD, how large a $\beta_C$ is needed to cancel out the causal effect of $X \rightarrow Y$.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
myDAG <- dagify(
  X ~ C,
  Y ~ C,
  Y ~ X,
  exposure = "X", # treatment
  outcome = "Y",
  coords = list(
    x = c(X = 1, C = 2, Y = 3),
    y = c(X = 1, C = 2, Y = 1)
  )
)

ggdag(myDAG) + theme_dag()
```

For the sake of completness we'll assume $\mathrm{SMD} = -2.3$ to validate the result in the previous section, i.e., we should end up with $\beta_C \approx 1.5$.

```{r}
tidy(withC, conf.int=TRUE) |>
        filter(term == "x") |>
        pull(estimate) |>
        tip(exposure_confounder_effect = -2.3)
```

Above we see that $\beta_C \approx 1.50$ (i.e., the confounder_outome_effect), thus we have an indication that this works both ways. Given that we have introduced two distinct concepts used to estimate unobserved/unknown confounders, we can now turn out attention to a more involved example.

# A second, more realistic, example

```{r, echo=FALSE, out.width="60%", fig.align="center"}
var_labs <- c(
  `P` = "P",
  `BA` = "BA",
  `H` = "H",
  `PL` = "PL",
  `L` = "L",
  `O` = "O",
  `PT` = "PT",
  `S` = "S",
  `TS` = "TS"
)

col_labs <- c(
  `P` = "black", `BA` = "black", `H` = "grey60", `PL` = "grey60",
  `L` = "grey60", `O` = "grey60", `PT` = "grey60", `S` = "grey60",
  `TS` = "grey60"
)

udag <- ggdag::dagify(
  H ~ BA,
  P ~ BA,
  P ~ H,
  PL ~ H,
  P ~ L,
  BA ~ O,
  L ~ O,
  P ~ O,
  P ~ PL,
  P ~ PT,
  P ~ S,
  TS ~ S,
  P ~ TS,
  exposure = "BA",
  outcome = "P"
)

df <- udag %>%
  node_canonical(seed = 7) %>%
  mutate(var_labs = var_labs[name], cols = col_labs[name])

ggplot(data = df, mapping = aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(mapping = aes(colour = name), show.legend = FALSE) +
  geom_dag_edges_link() +
  scale_colour_manual(values = df$data$cols) +
  geom_dag_text(mapping = aes(label = var_labs), parse = TRUE) + 
  theme_dag()
```

In [@feldtSFT23sysrev], the authors extracted causal links from several primary studies and assembled disparate evidence into a Unified Directed Acyclic Graph (UDAG). The purpose was to, in the future, analyze the direct effect of Business Area ($BA$) on Productivity, ($P$), i.e., Business Area can be seen as a "treatment" and Productivity as the outcome. 

If one would want to design the next observational study investigating this particular outcome and treatment, the idea would be that one could use the UDAG to know what variables to collect; after all, some studies contained some variables, but no study contained all variables in the UDAG. However, the UDAG, as we will see, can also be used for sensitivity analysis of unknown confunders. That would allow a future study to either be more sure of not facing omitted variable bias from start, i.e., the UDAG is likely to be correct, or to make sure to collect variables that are currently unobserved, i.e., the UDAG needs to be complemented with new variables.

The above UDAG is clearly a more involved example and contains several of the elemental constructs one can find in a DAG. Organization ($O$) is a confounder since it points to the treatment and the outcome. Hardware ($H$) is part of a pipe since $BA \rightarrow H \rightarrow P$. The lower part also contains several ancestors of the outcome. Additionally, we also see that there are no colliders (e.g., $X \rightarrow C \leftarrow Y$) in the UDAG. So, this seems to be a simple task. Let's see what we should condition on by calculating the adjustment set for the UDAG :

```{r}
adjustmentSets(udag, effect = "direct")
```

It seems our adjustment set is $\mathcal{A} = \{ H, O \}$. By conditioning on $H$ and $O$ (i.e., adjusting for $H$ and $O$) we will be able to estimate the direct effect of $BA$ on $P$. But what if there are unknown confounders? It's not unlikely given that the studies that contributed to the UDAG are all observational. 

Let's do this systematically and overlay numbers on the plot to make sure that we don't miss anything. First, we focus on the nine edges in the upper part of the UDAG since they are all connected to $BA$ directly or indirectly without needing to go through $P$.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
ggplot(data = df, mapping = aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(mapping = aes(colour = name), show.legend = FALSE) +
  geom_dag_edges_link() +
  scale_colour_manual(values = df$data$cols) +
  geom_dag_text(mapping = aes(label = var_labs), parse = TRUE) +
  annotate("text", x = 0, y = 1.17, label = "1") +
  annotate("text", x = 0.9, y = 1.2, label = "2") +
  annotate("text", x = 1.75, y = 0.7, label = "3") +
  annotate("text", x = 1.3, y = -0.1, label = "4") +
  annotate("text", x = -0.4, y = -0.15, label = "5") +
  annotate("text", x = 0.87, y = 0.4, label = "9") +
  annotate("text", x = 0.4, y = 0.4, label = "8") +
  annotate("text", x = -0.8, y = 0.6, label = "6") +
  annotate("text", x = -0.13, y = 0.3, label = "7") +
  theme_dag()
```

If we look at the plot above, and take #1 as an example, the question we're asking is: 

> If there would be a confounder $C$ between $H$ and $BA$, i.e. $H \leftarrow C \rightarrow BA$, how would that affect our possibility to estimate the direct effect of $BA$ on $P$?

The potential answer to the above question would be one of three: 

1. It would not affect the estimate (n/a).
2. By conditioning on other variable(s) we close the path and then the confounder will not affect the estimate (co).
3. We truly have a case of omitted variable bias, which means that we need to conduct a sensitivity analysis (ovb).

In the first case (n/a), a confounder would not affect the possibility for us to receive an unbiased causal estimate of the direct effect. In the second case (co), the confounder would affect our possibility to get an unbiased estimate; however, if we condition on $1,\ldots,n$ other variables, the confounder would no longer affect the results and thus fall under the n/a category. Finally, the third case (ovb), is the one we would not like to see. Here a potential confounder would affect our possibility to get an unbiased estimate. This means that we either need to add additional assumptions and argue that this confounder is unlikely to exist, or think harder, identify such a confounder and then measure it.

For each edge in the plot above, let's analyze what it would imply if an unknown confounder truly existed between the two nodes connected by the edge:

1. n/a
2. n/a
3. co $L$
4. n/a
5. n/a
6. co $PL$
7. ovb
8. ovb
9. n/a

How did we come to the results above? Well, analyzing the UDAG according to Pearl's causal calculus [@pearl09reason] leads to the above results and with experience one receives intuition when analyzing DAGs. However, one can also use software to get some help, e.g., the [dagitty](https://dagitty.net) package in R. If you are so inclined you can verify [online](https://dagitty.net/dags.html?id=P8mw_j) that our analysis above is correct.

To summarize, we can make life easier by also conditioning on $PL$ and $L$, i.e., $\mathcal{A} = \{ H, O, PL, L \}$. However, we still have a challenge dealing with #7 and #8. If an unknown confounder exists between the two nodes on each of those two edges, then we need to fall back on our sensitivity analysis of unknown confounders. However, before we do that, let's first look at the bottom half.

First, if there's a confounder $PT \leftarrow C \rightarrow BA$ or $PT \leftarrow C \rightarrow H$ then we are in trouble. But if we condition on $PT$ we close these paths and even gain higher accuracy in our causal estimate, since $PT$ is an ancestor of the outcome $P$. Hence, we should add $PT$ to our adjustment set. Next, imagine there's a confounder $S \leftarrow C \rightarrow BA$ or $S \leftarrow C \rightarrow H$. That would make it impossible for us to estimate the causal effect correctly. However, as in the previous case, conditioning on $S$ will close that path. Hence, $S$ should be added to our adjustment set. Finally, $TS$ should also be added to our adjustment set, for the same reasons as in the previous two cases.

Ultimately, this implies that if we want to estimate the direct causal effect of $BA$ on $P$, we need to condition on 

$\mathcal{A} = \{ H, O, PL, L, PT, S, TS \}$

to account for possible unknown confounders. However, remember, there's still a challenge in dealing with #7 and #8, i.e., in these two cases we need to analyze how large a confounder needs to be to nullify the direct causal effect of $BA$ on $P$. Let's do just that.

## Model design and execution

In addition to the outcome $P$ and the treatment $BA$, there are several variables we've decided to control (adjust) for. However, if conducting sensitivity analysis of unknown confounders, there are only two edges we need to care about (#7 and #8, which we concluded in the previous section).

Let's look at these two cases of potential omitted variable bias, and some implications of what an unknown confounder would be doing in this case.

```{r, echo=FALSE, fig.align="center"}
var_labs <- c(
  `P` = "P",
  `BA` = "BA",
  `H` = "H",
  `PL` = "PL",
  `L` = "L",
  `O` = "O",
  `PT` = "PT",
  `S` = "S",
  `TS` = "TS"
)

col_labs <- c(
  `P` = "black", `BA` = "black", `H` = "black", `PL` = "grey60",
  `L` = "grey60", `O` = "grey60", `PT` = "grey60", `S` = "grey60",
  `TS` = "grey60"
)

udag <- ggdag::dagify(
  H ~ BA,
  P ~ BA,
  P ~ H,
  PL ~ H,
  P ~ L,
  BA ~ O,
  L ~ O,
  P ~ O,
  P ~ PL,
  P ~ PT,
  P ~ S,
  TS ~ S,
  P ~ TS,
  exposure = "BA",
  outcome = "P"
)

df <- udag %>%
  node_canonical(seed = 7) %>%
  mutate(var_labs = var_labs[name], cols = col_labs[name])

ggplot(data = df, mapping = aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(mapping = aes(colour = name), show.legend = FALSE) +
  geom_dag_edges_link() +
  scale_colour_manual(values = df$data$cols) +
  geom_dag_text(mapping = aes(label = var_labs), parse = TRUE) + 
  annotate("text", x = 0.4, y = 0.4, label = "8") +
  annotate("text", x = -0.13, y = 0.3, label = "7") +
  theme_dag()
```

First, what we are talking about are two confounders: $BA \leftarrow C \rightarrow P$ and $H \leftarrow C \rightarrow P$. If there is a confounder there, then even if those two confounders would affect any other adjusted variable there'd be no need to worry&#8212;it won't affect our possibility to receive an unbiased estimate of $BA$'s effect on $P$ by the laws of causal calculus. But, let's break this down into the three possible ways a confounder could affect the estimation, if found between the aforementioned nodes.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
edge7 <- dagify(
  P ~ BA, 
  H ~ BA,
  P ~ C,
  H ~ C,
  P ~ H,
  coords = list(
    x = c(BA = 1, P = 1, H = 0.7, C = 0.95),
    y = c(BA = 2, P = 1, H = 1.75, C = 1.5)
  )
)
ggdag(edge7) + theme_dag()
```

What we see above (#7), is a confounder between $H$ and $P$. Such a confounder would seriously affect our possibility to receive an unbiased estimate of the direct effect of $BA$ on $P$, since the confounder would affect the outcome and $H$, which we chose to condition on.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
edge8 <- dagify(
  P ~ BA, 
  H ~ BA,
  BA ~ C,
  P ~ C,
  P ~ H,
  coords = list(
    x = c(BA = 1, P = 1, H = 0.7, C = 0.95),
    y = c(BA = 2, P = 1, H = 1.75, C = 1.5)
  )
)
ggdag(edge8) + theme_dag()
```

Above (#8) is a confounder between $BA$ and $P$. This is exactly as our first example we presented earlier. Since the confounder affects the treatment and outcome we are in trouble and it would not be possible to get an unbiased estimate.

```{r, echo=FALSE, out.width="60%", fig.align="center"}
edge7and8 <- dagify(
  P ~ BA, 
  H ~ BA,
  BA ~ C,
  P ~ C,
  H ~ C,
  P ~ H,
  coords = list(
    x = c(BA = 1, P = 1, H = 0.7, C = 0.95),
    y = c(BA = 2, P = 1, H = 1.75, C = 1.5)
  )
)
ggdag(edge7and8) + theme_dag()
```

Finally, above, we see a third example, which is a combination of the two previous ones, i.e., a confounder affecting all three nodes $\{H, BA, P\}$. However, if we do sensitivity analysis on the previous two cases above, we would implicitly cover this case. Hence, in summary, we are now fairly certain that the focus for our sensitivity analysis of unknown confounders should focus on the first two cases, and we should design a generative model with our treatment $BA$, our outcome $P$, and $H$ which we have adjusted for. With such a model, we can analyze the impact the two possible confounders can have when estimating the direct effect of $BA$ on $P$.

Hence, four variables are of interest to us:

* Business Area ($BA$), treatment: $\mathsf{Categorical}$ distribution; $\leq5$ cat.
* Hardware ($H$), adjusted: $\mathsf{Categorical}$ distribution; $\leq6$ cat.
* Productivity ($P$), outcome: Unit interval $[0,1]$, i.e., $\mathsf{Beta}$ distribution, with a $\mathsf{logit}$ link for the $\mu$ parameter and a $\mathsf{log}$ link for the $\phi$ parameter.
* Confounder ($C$): Assume $\mathbb{R}$, i.e., a $\mathsf{Normal}$ distribution.

Let's design a generative model including the variables above, but first without a confounder, to ensure that we can recover our parameter values. For this we'll use the excellent {simcausal} package [@simcausal].

```{r, echo=FALSE, out.width="60%", fig.align="center"}
edgeBasic <- dagify(
  P ~ BA, 
  H ~ BA,
  P ~ H,
  coords = list(
    x = c(BA = 1, P = 1, H = 0.7),
    y = c(BA = 2, P = 1, H = 1.75)
  )
)
ggdag(edgeBasic) + theme_dag()
```

```{r 1st-sim, cache=TRUE, warnings=FALSE, message=FALSE}
N <- 250 # Sample size

# Set beta effects
b_H <- 0.3
b_BA <- 0.2

D <- DAG.empty()

D <- D +
  # Set BA to categorical (c=3).
  # Make sure it starts at 1, to ensure sane priors later.
  node("BA", 
     distr = "rcat.b1",
     probs = c(0.5, 0.25, 0.25)) +
  
  # Set H to categorical (c=3), starting at 1
  # Make sure we include BA in H!
  node("H",
     distr = "rcat.b1",
     probs = (BA == 1) * c(0.7, 0.1, 0.2) + (BA == 2) * c(0.2, 0.1, 0.7) + (BA == 3) * c(0.2, 0.1, 0.7)) + 
  
  # Generate Beta(mu, phi) with H and BA.
  # Note: We use rbeta_mean() which is a mean parameterization of Beta.
  node("P",
     distr = "rbeta_mean",
     phi = 4,
     mu = inv_logit(b_H * H + b_BA * BA))

# Need to use inv_logit() since mu in Beta needs it when modelling
Dset <- set.DAG(D, vecfun  = "inv_logit")

# Gen N samples
d <- simcausal::sim(Dset, n = N)

# Set sane priors
p <- get_prior(P ~ H + BA, family = Beta, data = d)
p[1,1] <- "normal(0,1)"
p[4,1] <- "normal(0,2)"

# Run once so we can reuse model in sim
m <- brm(P ~ H + BA, family = Beta, data = d, prior = p, refresh = 0)

# Store results from all the runs
BA_hat <- rep(NA, N)

# Simulate
for(i in 1:N) {
  d <- simcausal::sim(Dset, n = N)
  m <- update(m, newdata = d, family = Beta, refresh = 0)
  BA_hat[i] <- fixef(m)[1] + fixef(m)[3]
}

# Margin of error 0.95
BA_err <- qt(0.975, df = N - 1) * sd(BA_hat)/sqrt(N)

# Lower and upper CI
BA_ci_l <- format(round(mean(BA_hat) - BA_err, 3), nsmall = 3)
BA_ci_u <- format(round(mean(BA_hat) + BA_err, 3), nsmall = 3)
BA_mu <- format(round(mean(BA_hat), 3), nsmall = 3)

print(paste0("BA=", b_BA, ", while 95% CI for simulated mu is: ", BA_mu, "[", BA_ci_l, ",", BA_ci_u, "]"))
# nice and tight
```

The simulation indicates (even when we add noise) that we can recover the parameters we set. Let's create a data set where we also have a confounder $C$, which we first condition on and then do not include it in the model. The confounder $C$ will affect both the outcome $P$ and the treatment $BA$.

```{r 2nd-sim, cache=TRUE, warning=FALSE, message=FALSE, results='hold'}
N <- 250 # Sample size

# Set beta effects
b_H <- 0.3
b_BA <- 0.2
b_C <- 0.5

D <- DAG.empty()

D <- D +
  # Confounder with Gaussian distribution
  node("C",
      distr = "rnorm",
      mean = 0,
      sd = 2) +

  # Set BA to categorical (c=3).
  # Make sure it starts at 1, to ensure sane priors later.
  # Let C be a causal effect on BA.
  node("BA", 
    distr = "rcat.b1",
    probs = softmax(C * c(0.5, 0.25, 0.25))) + 

  # Set H to categorical (c=3).
  # Let BA be a causal effect on H.
  node("H",
    distr = "rcat.b1",
    probs = (BA == 1) * c(0.7, 0.1, 0.2) + (BA == 2) * c(0.2, 0.1, 0.7) + (BA == 3) * c(0.2, 0.1, 0.7)) + 

  # Generate Beta(mu, phi) with H, BA, and C.
  # Note: We use rbeta_mean() which is a mean parameterization of Beta.
  node("P",
    distr = "rbeta_mean",
    phi = 4,
    mu = inv_logit(b_H * H + b_BA * BA + b_C * C))

# Need to use inv_logit() since mu in Beta needs it when modelling
Dset <- set.DAG(D, vecfun  = c("inv_logit", "softmax"))

# Gen N samples
d <- simcausal::sim(Dset, n = N)
d$P[d$P == 1] <- 0.99999999 # nudge iff 1

# Set sane priors
p <- get_prior(P ~ H + BA + C, family = Beta, data = d)
p[1,1] <- "normal(0,1)"
p[5,1] <- "normal(0,2)"

# Run once so we can reuse model in sim
m <- brm(P ~ H + BA + C, family = Beta, data = d, prior = p, refresh = 0, backend = "rstan")

# Store results from all the runs
BA_hat <- rep(NA, N)

# Simulate
for(i in 1:N) {
  d <- simcausal::sim(Dset, n = N)
  d$P[d$P == 1] <- 0.99999999

  # update model using new data
  m <- update(m, newdata = d, family = Beta, refresh = 0)

  # store coefficients
  BA_hat[i] <- fixef(m)[1] + fixef(m)[3]
}

# Margin of error 0.95
BA_err <- qt(0.975, df = N - 1) * sd(BA_hat)/sqrt(N)

# Lower and upper CI
BA_ci_l <- format(round(mean(BA_hat) - BA_err, 3), nsmall = 3)
BA_ci_u <- format(round(mean(BA_hat) + BA_err, 3), nsmall = 3)

# mean
BA_mu <- format(round(mean(BA_hat), 3), nsmall = 3)

print(paste0("BA=", b_BA, ", while 95% CI for simulated mu (when including C) is: ", BA_mu, "[", BA_ci_l, ",", BA_ci_u, "]"))

###############################################################################
############### and w/o the confounder to see that it all works ###############
###############################################################################

# Gen N samples
d <- simcausal::sim(Dset, n = N)
d$P[d$P == 1] <- 0.99999999

# Set sane priors
p <- get_prior(P ~ H + BA, family = Beta, data = d)
p[1,1] <- "normal(0,1)"
p[4,1] <- "normal(0,2)"

# Run once so we can reuse model in sim
m <- brm(P ~ H + BA, family = Beta, data = d, prior = p, refresh = 0, backend = "rstan")

# Store results from all the runs
BA_hat <- rep(NA, N)

# Simulate
for(i in 1:N) {
  d <- simcausal::sim(Dset, n = N)
  d$P[d$P == 1] <- 0.99999999

  # update model using new data
  m <- update(m, newdata = d, family = Beta, refresh = 0)

  # store coefficients
  BA_hat[i] <- fixef(m)[1] + fixef(m)[3]
}

# Margin of error 0.95
BA_err <- qt(0.975, df = N - 1) * sd(BA_hat)/sqrt(N)

# Lower and upper CI
BA_ci_l <- format(round(mean(BA_hat) - BA_err, 3), nsmall = 3)
BA_ci_u <- format(round(mean(BA_hat) + BA_err, 3), nsmall = 3)

# mean
BA_mu <- format(round(mean(BA_hat), 3), nsmall = 3)

print(paste0("BA=", b_BA, ", while 95% CI for simulated mu (when not including C) is: ", BA_mu, "[", BA_ci_l, ",", BA_ci_u, "]"))
```

Above we saw, in the first case, that when we condition on $C$ we also get correct estimates of $BA$, and just as in the early example, when we don't condition on $C$ we get a biased estimate of $BA$. Next, we need to decide on different effect sizes of $BA \rightarrow P$, and then analyze what size a confounder, $C$, need to be to invalidate those effects. After that we do the same for the confounder affecting $P$ and $H$ to see if we can see any differences.

## Sensitivity analysis of unknown confounders
<!-- TODO: Remember to do coeff/(1-coeff) to transform Beta reg coef to OR, before tipping results -->

# References
